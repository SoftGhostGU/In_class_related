## 2025国赛模型准备

### 001 基于熵权法的TOPSIS模型

- 评价模型
- TOPSIS不太会作为主要算法，只会成为其他模型的辅助模型

1. 首先使用熵权法确定各指标的权重
   1. 标准化处理
   2. 计算熵值 $e_j$
   3. 计算权重 $w_j$
2. 然后采用TOPSIS方法对备选方案进行评价和排序
   1. 计算加权标准化决策矩阵 $v_{ij}=w_j·r_{ij}$
   2. 确定正理想解和负理想解
   3. 计算各个方案与正负理想值的距离
   4. 计算相对接近度 $c_i$ 越高越好

可以在正负理想解上进行创新：
- 比如需要的值不是最大or最小，而是某个区间？
- 比如数值只是在某个季节最好，而不是全年最好？

示例中的代码仅有x1,x2,x3三个指标，实际情况可能有更多指标
- 需要进行相应的修改
- 或者通过降维操作来简化问题

### 002 灰色关联综合评价

- 评价模型
- 与TOPSIS模型类似，仍然只能作为辅助模型 

> 主要用于衡量多个系统或指标之间的**相似性或关联性**。灰色关联分析通过比较系统中各个因素的变化趋势，来判断不同因素之间的关联程度，进而评价系统的整体表现。

1. 数据标准化 $r_{ij}=\frac{x_{ij}}{max(x_{·j})}$
2. 计算关联度
   1. 计算差异序列
   2. 计算最大差异和最小差异
   3. 计算灰色关联度 $\frac{\triangle _{min}+\rho \triangle _{max}}{\triangle _{ij}+\rho \triangle _{max}}(\rho=0.5)$
   4. 计算灰色关联度的均值

创新点：
- 灰色关联度并没有考虑到各指标的权重，因此可以考虑引入权重
- 类似地，可以考虑聚类之后再计算灰色关联度
- 可以考虑将灰色关联与TOPSIS对比，看看哪种方法更好

### 003 模糊综合评价

- 评价模型
- 与灰色关联综合评价模型类似，仍然只能作为辅助模型
- 需要自己收集数据（专家or问卷）

> 主要用于处理那些**具有模糊性和不确定性的评价**问题（eg. 问卷中的优良中差）。它通过模糊数学中的隶属度和模糊集理论，将复杂的、模糊的评价因素综合为一个定量的结果，广泛应用于教育、环境评价、质量评估等领域。

1. 确定评价指标体系 $\{U_1, U_2, U_3,..., U_n\}$
2. 建立模糊评价矩阵（专家打分法、模糊隶属函数法）
3. 确定权重向量 $w_j$
4. 进行模糊综合运算

创新点：
- 评价体系可以从他人论文中获得，或者自己降维后成为自己的评价体系
- 每个体系的权重可以自己设定或降维后得到
- 评价体系的解释性

### 004 AHP层次分析

- 评价模型
- 该算法自带权重和数据计算
- 与模糊综合评价模型类似，仍然需要自己收集数据

1. 获取准则矩阵（使用1-9量表）
2. 获取每个准则下备选方案的判断矩阵
3. 计算权重向量
4. 一致性检验（CI与CR，CR<0.1则认为一致）
5. 综合评价

> 通过对决策问题进行层次分解，将复杂的决策过程转化为一系列**两两比较的判断过程**，并通过计算权重和一致性检验来确定各方案的优先级。AHP 适用于解决决策问题中多目标、多方案且存在不确定性的情况，尤其是在专家判断或主观决策领域中具有广泛的应用。

### 005 Kmeans聚类

- 聚类模型
- 聚类之后可以对于每一类分别预测或评价

1. 随机选择K个中心点
2. 计算每个样本到K个中心点的距离，将点分配到中心点所属的簇
3. 计算每个簇的平均值，更新中心的
4. 重复2-3步，直到中心点不再变化或达到最大迭代次数

数据可视化：
- 如果数据是二维的，可以用散点图表示
- 如果数据是多维的，那么使用t-SNE降维后绘制

### 006 光谱聚类

- 聚类模型
- 数学建模比赛中不注重怎么聚类，只注重结果

1. 构建相似性矩阵
2. 构建拉普拉斯矩阵
3. 特征值分解
4. Kmeans聚类

> 光谱聚类（Spectral Clustering）是一种基于图论的聚类算法，利用数据点之间的相似性或距离信息，将数据点表示为图结构，并通过图的拉普拉斯矩阵的特征值分解来实现聚类。光谱聚类可以在处理复杂的几何形状数据、非凸数据集和其他传统聚类方法（如 KMeans）无法有效处理的数据时表现出色。

光谱聚类不常用，通常用于具有复杂几何图形的聚类问题（例如示例代码中的月牙形图案）。

### 007 高斯混合模型聚类

- 聚类模型
- 前两个聚类返回一个点属于哪个类，而该模型返回一个点属于一个类的概率大小

> 假设数据点来自若干个不同的高斯分布。与 KMeans 聚类不同，高斯混合模型不仅考虑到簇中心，还考虑到每个簇的形状和分布。因此，GMM 可以适应更复杂的簇形状，如椭圆形或非对称形状，适用于许多实际应用中的复杂聚类任务。

### 008 DBSCAN聚类

- 聚类模型
- 无需提前指定簇数，根据密度来确定
- 在某些情况下这个聚类方法并不适用

DBSCAN 的主要思想是通过数据点的密度来定义簇，具体来说，它定义了以下三类点：
1. **核心点（Core Point）**：如果某个点的邻域内有足够多的点（即在半径 $\varepsilon$ 内至少有 $MinPts$ 个点），则该点被认为是核心点。
2. **边界点（Border Point）**：如果某个点的邻域内点数不足 $MinPts$，但它在某个核心点的邻域内，则它被称为边界点。
3. **噪声点（Noise Point）**：如果某个点既不是核心点，也不是边界点，则被视为噪声点。

> 一种基于密度的聚类算法，它通过考察数据点在空间中的密度来识别簇和噪声点。与 KMeans 和高斯混合模型不同，DBSCAN 不需要预先指定簇的数量，并且能够发现**任意形状的簇**，尤其**适用于处理具有噪声和不规则形状**的复杂数据。

### 009 均值漂移聚类

- 聚类模型
- 无需提前指定簇数，根据密度来确定

均值漂移的基本概念
1. **核密度估计（Kernel Density Estimation, KDE）**：均值漂移算法通过核密度估计来估算数据点的概率密度分布。对于一个数据点 $x_i$，其密度通过一个核函数来计算。常见的核函数是高斯核函数。
2. **均值漂移**：均值漂移是指对于每个数据点，根据它周围点的加权平均位置，逐步移动该点的过程。每次迭代后，数据点会向更高密度的区域移动，直到收敛到密度峰值。
3. **窗口带宽（Bandwidth, $h$）**：在均值漂移聚类中，带宽 $h$ 是一个非常重要的参数，它决定了核函数的范围，即在哪个范围内搜索邻居。窗口带宽的选择对最终的聚类效果有很大影响。

> 通过对数据点进行密度估计并迭代移动数据点，最终使数据点集中在高密度区域，从而形成簇。均值漂移聚类不需要预先指定簇的数量，并且能够识别任意形状的簇。

> 其原理是基于**核密度估计**，可以视为一种**非参数化的模式寻找算法**。

### 010 传播聚类

- 聚类模型
- 无需提前指定簇数，可以处理任意形状的簇

1. 相似性计算
2. 初始化责任与可用性
3. 责任更新
4. 可用性更新
5. 迭代条件
6. 确定簇中心和分配簇

> 不需要预先指定簇的数量，也不依赖随机初始化簇中心。传播聚类通过消息传递（message passing）在数据点之间交换信息，从而自动选出一部分数据点作为簇的代表，称为代表点（Exemplars），並將其他数据点归到这些代表点的簇中。

### 011 决策树回归

- 回归模型
- 可以在聚类后分别回归

1. 输入数据
2. 遍历特征$X_j$
3. 根据每个划分点$t$，将数据集划分为多个子集$R_n(j,t)$
4. 计算每个划分点的均方误差（MSE）
5. 再次划分

创新：
- 可以通过对于输入集的控制变量，来判断每个输入对于结果的变化权重

### 012 随机森林回归

- 回归模型
- 相当于多个决策树的集成，虽然部分决策树可能过拟合，但是其在离散的分类任务下表现很好
- 可以对于所有的回归模型做解释

1. 输入数据
2. 对每一个子样本构建一颗决策树，使用单棵决策树来划分和预测
3. 根节点，内部节点，叶子节点分别操作
4. 最终结果

### 013 逻辑回归

- 回归模型
- 逻辑回归是一种分类模型，可以用于二分类问题
- 后续可以作为多分类模型的基础，例如神经网络等

1. 输入数据
2. 线性模型
3. sigmoid 函数
4. 分类决策
5. 损失函数来优化模型参数

> 一种广泛应用的分类算法，尽管名字带有“回归”二字，但它主要用于解决二分类问题。它通过将输入特征映射到一个概率值，进而将样本分类为某一个类别。逻辑回归不仅仅适用于二分类问题，也可以通过扩展应用于多分类问题。

### 014 支持向量机

- 分类模型
- 支持向量机是一种二类分类模型，可以用于线性可分数据集

> 一种广泛应用于分类、回归和异常检测任务的监督学习模型，尤其在处理二分类问题时表现出色。 SVM 的核心思想是找到一个能够最大化类间间隔 (Margin) 的超平面，以便对不同类别进行分类。

### 015 非线性支持向量机

- 分类模型
- 非线性支持向量机是一种支持向量机的扩展，可以处理非线性数据集

> 在许多实际应用中，数据集并不是线性可分的。例如，在二维平面上，数据的分布可能表现为曲线形状，或者是更复杂的模式。这时，我们就需要用到非线性支持向量机（Non-linear SVM）。它通过核技巧（Kernel Trick），将低维空间中的非线性问题映射到高维空间中，从而在**高维空间找到线性可分的超平面**。

### 016 KNN分类器

- 分类模型
- KNN分类器是一种简单而有效的分类算法，不需要训练过程，适合**多分类**问题

1. 选择K的值
2. 计算待分类样本与各个训练样本之间的距离
3. 确定K个最近邻居
4. 投票决定分类

> 一种基于实例的监督学习算法，用于分类和回归任务。KNN的核心思想是：对于一个待分类的样本，找到训练集中**距离它最近的K个邻居**，根据这些邻居的类别，通过多数投票的方式来确定待分类样本的类别。KNN属于懒惰学习算法，因为它在训练阶段不进行任何建模，而是**在预测阶段才使用整个训练集**。

### 017 主成分分析

- 降维模型，通常与其他模型一起使用
- 主成分分析（PCA）是一种无监督学习算法，可以用于降维

1. 数据标准化
2. 计算协方差矩阵
3. 计算特征值和特征向量
4. 选取前K个特征向量
5. 数据投影

> 一种线性降维技术，旨在减少数据的维度，同时尽可能保留数据的方差（即信息）。PCA 的核心思想是通过寻找新的坐标轴（主成分），使得数据在这些新坐标轴上的投影能够尽可能地解释原始数据的方差。PCA 通常用于数据降维、去噪、特征提取和可视化。

### 018 因子分析

- 降维模型
- 减少数据的维度，揭示隐藏在多个观测变量背后的潜在结构

> 通过寻找潜在的少数几个因子，来解释原始数据中的观测变量之间的相关性。因子分析与主成分分析（PCA）相似，都是降维技术，但它更侧重于解释变量之间的共性结构，而不是仅仅减少维度。

### 019 T分布随机嵌入（t-SNE）

- 降维模型
- T分布是将高维数据和低维数据尽可能接近

1. 高维空间中的邻域相似度计算
2. 初始化低维空间嵌入
3. 最小化KL散度

> 目标是将高维数据嵌入到一个低维空间中，同时尽可能保持高维空间中数据点之间的相对距离。**更适合处理非线性数据结构**。它通过将高维数据点在低维空间中进行近似保留邻域关系，最终生成一个可以用作可视化的低维数据分布。

### 020 核主成分分析（KPCA）

- 降维模型
- 利用核技巧将数据从原始输入映射到高维特征空间中，再进行主成分分析

1. 计算核矩阵
2. 中心化核矩阵
3. 特征值分解
4. 投影到主成分

> 利用了核技巧（Kernel Trick）将数据从原始的输入空间映射到一个高维特征空间中，在高维空间中进行线性主成分分析，进而捕捉数据中的非线性结构。

### 021 线性规划

- 优化模型
- 线性规划是一种求解线性规划问题的数学方法，约束条件和目标函数往往是线性的

1. 线性约束条件
2. 目标函数
3. 解的存在性
4. 解的唯一性

> 在某些约束条件下找到一个线性目标函数的最优值（最大值或最小值）。线性规划广泛应用于经济、管理、工程等领域，解决资源分配、生产调度、运输问题等实际问题。

### 022 整数规划

- 优化模型
- A题中出现较多，约束变量是整数的

> 不仅要求约束条件和目标函数是线性的，还要求所有或者部分变量只能取整数值。

### 023 0-1规划

- 优化模型
- 通常解决是或不是的离散问题

1. 决策变量
2. 目标函数
3. 约束条件

> 适合处理许多离散优化问题，尤其是在选择、指派、调度等场景下有广泛的应用。

创新点：
- 在于求解方法：分支定界法、隔平面法、动态规划、启发式算法和元启发式算法等。
- 模拟退火、遗传算法、粒子群优化算法、蚁群算法等。
- 新算法：黏菌算法、真菌算法、共生算法、差分算法

### 024 混合整数线性规划

- 优化模型
- 同时处理离散决策和连续决策的问题，例如资源分配问题

1. 决策变量
2. 目标函数
3. 约束条件
4. 整数约束

> 一部分决策变量必须取整数值（如 0、1、2、3 等），而另一部分决策变量可以取连续值（即实数）。这种灵活性使得混合整数线性规划能够同时处理离散和连续决策问题

### 025 非线性规划

- 优化模型
- 非线性约束条件和目标函数

1. 决策变量
2. 目标函数
3. 约束条件
4. 非负条件

> 结合了线性规划和整数规划的特点，允许部分决策变量为整数，而另一部分决策变量可以为连续值。这种灵活性使得混合整数线性规划能够处理更加复杂的混合离散和连续决策问题

常见求解方法：
- 梯度下降法
- 拉格朗日乘数法
- KKT条件法
- 罚函数法（在目标函数中加入一个罚项，使得目标函数不可能达到其极小值）

灵敏度分析：
- 改变某个变量的系数，观察目标函数的变化，若目标函数的变化幅度较大，则说明该变量对目标函数的影响较大；若变化幅度较小，则说明该变量对目标函数的影响不大。
- 改变初始猜测，或许会影响结果产生的效率与结果本身，这也可以作为灵敏度分析的一种手段

### 026 二次规划

- 优化模型
- 目标函数是二次的，而约束条件是线性的；公式里系数建议使用矩阵来写

1. 决策变量
2. 目标函数
3. 约束条件
4. 求解（罚函数法、启发式算法和元启发式算法）

示例代码是黏菌算法的二次规划求解


